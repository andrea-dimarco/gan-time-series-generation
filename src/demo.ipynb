{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Generation using Contrastive Learning\n",
    "\n",
    "Consider learning a generative model for time-series data.\n",
    "\n",
    "The sequential setting poses a unique challenge: Not only should the generator capture the conditional dynamics of (stepwise) transitions, but its open-loop rollouts should also preserve the joint distribution of (multi-step) trajectories.\n",
    "\n",
    "On one hand, autoregressive models\n",
    "trained by MLE allow learning and computing explicit transition distributions, but suffer from compounding error during rollouts.\n",
    "\n",
    "On the other hand, adversarial models based on GAN training alleviate such exposure bias, but transitions are implicit and hard to assess.\n",
    "\n",
    "In this work, we study a generative framework that seeks to combine the strengths of both: Motivated by a moment-matching objective to mitigate\n",
    "compounding error, we optimize a local (but forward-looking) *transition policy*, where the reinforcement signal is provided by a global (but stepwise-decomposable) *energy model* trained by contrastive estimation. \n",
    "\n",
    "At **training**, the two components are learned cooperatively, avoiding the instabilities typical of adversarial objectives. \n",
    "\n",
    "At **inference**, the learned policy serves as the generator for iterative sampling, and the learned energy serves as a trajectory-level measure for evaluating sample quality.\n",
    "\n",
    "By expressly training a policy to imitate sequential behavior of time-series features in a dataset, this approach embodies *“generation by imitation”*. Theoretically, we illustrate the correctness of this formulation and the consistency of the algorithm.\n",
    "\n",
    "Empirically, we evaluate its ability to generate predictively useful samples from real-world datasets, verifying that it performs at the standard of existing benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install libraries\n",
    "\n",
    "Run the cell below to **install** the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb\n",
    "%pip install pytorch-lightning\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change or remove these commands with the right ones for your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install cuda-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n",
    "\n",
    "Run the cell below to **import** the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameters import Config\n",
    "import utilities as ut\n",
    "import dataset_handling as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eh eh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Hyper-parameters\n",
    "\n",
    "The cell below contains *all* the hyper-parameters nedded by this script, for easy tweaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment this cell if you don't want to use Weights & Biases to log the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Initialization\n",
    "\n",
    "Initialize the modules needed by running the cells in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.set_seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to the folder containing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = hparams.dataset_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the dataset as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import iid_sequence_generator, sine_process, wiener_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hparams.dataset_name in ['sine', 'wien', 'iid', 'cov']:\n",
    "  # Generate and store the dataset as requested\n",
    "  dataset_path = f\"../datasets/{hparams.dataset_name}_generated_stream.csv\"\n",
    "  if hparams.dataset_name == 'sine':\n",
    "    sine_process.save_sine_process(p=hparams.data_dim, N=hparams.num_samples, file_path=dataset_path)\n",
    "  elif hparams.dataset_name == 'wien':\n",
    "    wiener_process.save_wiener_process(p=hparams.data_dim, N=hparams.num_samples, file_path=dataset_path)\n",
    "  elif hparams.dataset_name == 'iid':\n",
    "    iid_sequence_generator.save_iid_sequence(p=hparams.data_dim, N=hparams.num_samples, file_path=dataset_path)\n",
    "  elif hparams.dataset_name == 'cov':\n",
    "    iid_sequence_generator.save_cov_sequence(p=hparams.data_dim, N=hparams.num_samples, file_path=dataset_path)\n",
    "  else:\n",
    "    raise ValueError\n",
    "  print(f\"The {hparams.dataset_name} dataset has been succesfully created and stored into:\\n\\t- {dataset_path}\")\n",
    "elif hparams.dataset_name == 'real':\n",
    "  pass\n",
    "else:\n",
    "  raise ValueError(\"Dataset not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hparams.dataset_name in ['sine', 'wien', 'iid', 'cov']:\n",
    "    train_dataset_path = f\"{datasets_folder}{hparams.dataset_name}_training.csv\"\n",
    "    test_dataset_path = f\"{datasets_folder}{hparams.dataset_name}_testing.csv\"\n",
    "    val_dataset_path  = f\"{datasets_folder}{hparams.dataset_name}_validating.csv\"\n",
    "\n",
    "    # Train & Test\n",
    "    dh.train_test_split(X=np.loadtxt(dataset_path, delimiter=\",\", dtype=np.float32),\n",
    "                    split=hparams.train_test_split,\n",
    "                    train_file_name=train_dataset_path,\n",
    "                    test_file_name=test_dataset_path    \n",
    "                    )\n",
    "\n",
    "    # Train & Validation\n",
    "    dh.train_test_split(X=np.loadtxt(train_dataset_path, delimiter=\",\", dtype=np.float32),\n",
    "                    split=hparams.train_val_split,\n",
    "                    train_file_name=train_dataset_path,\n",
    "                    test_file_name=val_dataset_path    \n",
    "                    )\n",
    "    \n",
    "    print(f\"The {hparams.dataset_name} dataset has been split successfully into:\\n\\t- {train_dataset_path}\\n\\t- {val_dataset_path}\")\n",
    "elif hparams.dataset_name == 'real':\n",
    "    train_dataset_path = datasets_folder + hparams.train_file_name\n",
    "    test_dataset_path  = datasets_folder + hparams.test_file_name\n",
    "    val_dataset_path   = datasets_folder + hparams.val_file_name\n",
    "else:\n",
    "  raise ValueError(\"Dataset not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the TimeGAN model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timegan_model import TimeGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Train\n",
    "\n",
    "This chapter will train the model according to the hyper-parameters defined above in section [Hyper-parameters](#13-hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_loop import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(datasets_folder=datasets_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import testing_loop as test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegan = TimeGAN(hparams=hparams,\n",
    "                    train_file_path=train_dataset_path,\n",
    "                    val_file_path=val_dataset_path\n",
    "                    )\n",
    "timegan.load_state_dict(torch.load(f\"./timegan-{hparams.dataset_name}.pth\"))\n",
    "\n",
    "timegan.eval()\n",
    "print(f\"TimeGAN {hparams.dataset_name} model loaded and ready for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dh.RealDataset(\n",
    "                file_path=test_dataset_path,\n",
    "                seq_len=hparams.seq_len\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sequence Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rec_loss = test.recovery_seq_test(model=timegan,\n",
    "                                      test_dataset=test_dataset,\n",
    "                                      limit=hparams.limit,\n",
    "                                      frequency=hparams.pic_frequency\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_gen_loss = test.generate_seq_test(model=timegan,\n",
    "                                      test_dataset=test_dataset,\n",
    "                                      limit=hparams.limit,\n",
    "                                      frequency=hparams.pic_frequency\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Complete Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.generate_stream_test(model=timegan,\n",
    "                          test_dataset=test_dataset,\n",
    "                          limit=hparams.limit,\n",
    "                          folder_path=\"./test_results/\",\n",
    "                          save_pic=True,\n",
    "                          compare=False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.distribution_visualization(model=timegan,\n",
    "                                test_dataset=test_dataset,\n",
    "                                limit=hparams.limit,\n",
    "                                folder_path=\"./test_results/\",\n",
    "                                save_pic=True\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.predictive_test(model=timegan,\n",
    "                     test_dataset=test_dataset,\n",
    "                     test_dataset_path=test_dataset_path,\n",
    "                     folder_path=\"./test_results/\",\n",
    "                     save_pic=True,\n",
    "                     limit=hparams.limit\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Linear Deterministic Anomaly Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these tests the model will be asked to generate sequences that a deterministic PCA-based anomaly detector will scan looking for irregularities with respect to the real sequences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAR, TAR = test.AD_tests(model=timegan, test_dataset=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
