{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Generation using Contrastive Learning\n",
    "\n",
    "Consider learning a generative model for time-series data.\n",
    "\n",
    "The sequential setting poses a unique challenge: Not only should the generator capture the conditional dynamics of (stepwise) transitions, but its open-loop rollouts should also preserve the joint distribution of (multi-step) trajectories.\n",
    "\n",
    "On one hand, autoregressive models\n",
    "trained by MLE allow learning and computing explicit transition distributions, but suffer from compounding error during rollouts.\n",
    "\n",
    "On the other hand, adversarial models based on GAN training alleviate such exposure bias, but transitions are implicit and hard to assess.\n",
    "\n",
    "In this work, we study a generative framework that seeks to combine the strengths of both: Motivated by a moment-matching objective to mitigate\n",
    "compounding error, we optimize a local (but forward-looking) *transition policy*, where the reinforcement signal is provided by a global (but stepwise-decomposable) *energy model* trained by contrastive estimation. \n",
    "\n",
    "At **training**, the two components are learned cooperatively, avoiding the instabilities typical of adversarial objectives. \n",
    "\n",
    "At **inference**, the learned policy serves as the generator for iterative sampling, and the learned energy serves as a trajectory-level measure for evaluating sample quality.\n",
    "\n",
    "By expressly training a policy to imitate sequential behavior of time-series features in a dataset, this approach embodies *“generation by imitation”*. Theoretically, we illustrate the correctness of this formulation and the consistency of the algorithm.\n",
    "\n",
    "Empirically, we evaluate its ability to generate predictively useful samples from real-world datasets, verifying that it performs at the standard of existing benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install libraries\n",
    "\n",
    "Run the cell below to **install** the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.15.0.post1\n",
      "Uninstalling tensorflow-2.15.0.post1:\n",
      "  Would remove:\n",
      "    /home/dima/.local/bin/estimator_ckpt_converter\n",
      "    /home/dima/.local/bin/import_pb_to_tensorboard\n",
      "    /home/dima/.local/bin/saved_model_cli\n",
      "    /home/dima/.local/bin/tensorboard\n",
      "    /home/dima/.local/bin/tf_upgrade_v2\n",
      "    /home/dima/.local/bin/tflite_convert\n",
      "    /home/dima/.local/bin/toco\n",
      "    /home/dima/.local/bin/toco_from_protos\n",
      "    /home/dima/.local/lib/python3.10/site-packages/tensorflow-2.15.0.post1.dist-info/*\n",
      "    /home/dima/.local/lib/python3.10/site-packages/tensorflow/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install pytorch-lightning\n",
    "# !pip install pyyaml\n",
    "# !pip install torchvision\n",
    "# !pip install plotly\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n",
    "\n",
    "Run the cell below to **import** the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Sequence, List, Dict, Tuple, Optional, Any, Set, Union, Callable, Mapping\n",
    "# import itertools\n",
    "\n",
    "# import dataclasses\n",
    "# from dataclasses import dataclass\n",
    "# from dataclasses import asdict\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "\n",
    "# from PIL import Image\n",
    "# import PIL\n",
    "\n",
    "# import torchvision.utils\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.datasets import MNIST\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torch import nn, optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import wandb\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "## Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. TimeGAN model\n",
    "from timegan import timegan\n",
    "# 2. Data loading\n",
    "from data_loading import real_data_loading, sine_data_generation\n",
    "# 3. Metrics\n",
    "from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "from metrics.predictive_metrics import predictive_score_metrics\n",
    "from metrics.visualization_metrics import visualization\n",
    "\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Hyper-parameters\n",
    "\n",
    "The cell below contains *all* the hyper-parameters nedded by this script, for easy tweaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1.0 # . . . Domain bounds for the loss functions\n",
    "M = 32 #. . . . Mini-batch size\n",
    "lr = 0.0007 # . Learning Rate\n",
    "k = 1.0 # . . . Regularization coefficient \n",
    "\n",
    "## Data loading\n",
    "data_name = 'stock' # . . which dataset to use\n",
    "seq_len = 24 #. . . . . . max length of the input sequence\n",
    "\n",
    "## Newtork parameters\n",
    "module = 'gru' #. . . . . Can be 'gru', 'lstm' or 'lstmLN'\n",
    "hidden_dim = 24 # . . . . Hidden dimensions\n",
    "num_layer = 3 # . . . . . Number of layers\n",
    "iterations = 10000 #. . . Number of epochs\n",
    "batch_size = 128 #. . . . Amount of samples in each batch\n",
    "\n",
    "metric_iteration = 5 #. . Number of iteration for each metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False # . . will require login for Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a function to count the number of parameters\n",
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "  \"\"\" Counts the number of trainable parameters of a module\n",
    "\n",
    "  :param model: model that contains the parameters to count\n",
    "  :returns: the number of parameters in the model\n",
    "  \"\"\"\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Initialization\n",
    "\n",
    "Initialize the modules needed by running the cells in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "_ = pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb:\n",
    "    !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_name in ['stock', 'energy']:\n",
    "  ori_data = real_data_loading(data_name, seq_len)\n",
    "elif data_name == 'sine':\n",
    "  # Set number of samples and its dimensions\n",
    "  no, dim = 10000, 5\n",
    "  ori_data = sine_data_generation(no, seq_len, dim)\n",
    "else:\n",
    "  assert(False)\n",
    "    \n",
    "print(data_name + ' dataset has been loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newtork parameters\n",
    "parameters = dict()\n",
    "\n",
    "parameters['module'] = module \n",
    "parameters['hidden_dim'] = hidden_dim\n",
    "parameters['num_layer'] = num_layer\n",
    "parameters['iterations'] = iterations\n",
    "parameters['batch_size'] = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Train\n",
    "\n",
    "This chapter will train the model according to the hyper-parameters defined above in section [Hyper-parameters](#13-hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
