{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Generation using Contrastive Learning\n",
    "\n",
    "Consider learning a generative model for time-series data.\n",
    "\n",
    "The sequential setting poses a unique challenge: Not only should the generator capture the conditional dynamics of (stepwise) transitions, but its open-loop rollouts should also preserve the joint distribution of (multi-step) trajectories.\n",
    "\n",
    "On one hand, autoregressive models\n",
    "trained by MLE allow learning and computing explicit transition distributions, but suffer from compounding error during rollouts.\n",
    "\n",
    "On the other hand, adversarial models based on GAN training alleviate such exposure bias, but transitions are implicit and hard to assess.\n",
    "\n",
    "In this work, we study a generative framework that seeks to combine the strengths of both: Motivated by a moment-matching objective to mitigate\n",
    "compounding error, we optimize a local (but forward-looking) *transition policy*, where the reinforcement signal is provided by a global (but stepwise-decomposable) *energy model* trained by contrastive estimation. \n",
    "\n",
    "At **training**, the two components are learned cooperatively, avoiding the instabilities typical of adversarial objectives. \n",
    "\n",
    "At **inference**, the learned policy serves as the generator for iterative sampling, and the learned energy serves as a trajectory-level measure for evaluating sample quality.\n",
    "\n",
    "By expressly training a policy to imitate sequential behavior of time-series features in a dataset, this approach embodies *“generation by imitation”*. Theoretically, we illustrate the correctness of this formulation and the consistency of the algorithm.\n",
    "\n",
    "Empirically, we evaluate its ability to generate predictively useful samples from real-world datasets, verifying that it performs at the standard of existing benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install libraries\n",
    "\n",
    "Run the cell below to **install** the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.15.0.post1\n",
      "Uninstalling tensorflow-2.15.0.post1:\n",
      "  Would remove:\n",
      "    /home/dima/.local/bin/estimator_ckpt_converter\n",
      "    /home/dima/.local/bin/import_pb_to_tensorboard\n",
      "    /home/dima/.local/bin/saved_model_cli\n",
      "    /home/dima/.local/bin/tensorboard\n",
      "    /home/dima/.local/bin/tf_upgrade_v2\n",
      "    /home/dima/.local/bin/tflite_convert\n",
      "    /home/dima/.local/bin/toco\n",
      "    /home/dima/.local/bin/toco_from_protos\n",
      "    /home/dima/.local/lib/python3.10/site-packages/tensorflow-2.15.0.post1.dist-info/*\n",
      "    /home/dima/.local/lib/python3.10/site-packages/tensorflow/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install wandb\n",
    "# !pip install pytorch-lightning\n",
    "# !pip install pyyaml\n",
    "# !pip install torchvision\n",
    "# !pip install plotly\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n",
    "\n",
    "Run the cell below to **import** the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, List, Dict, Tuple, Optional, Any, Set, Union, Callable, Mapping\n",
    "import itertools\n",
    "\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "# from pprint import pprint\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "\n",
    "# from PIL import Image\n",
    "# import PIL\n",
    "\n",
    "# import torchvision.utils\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms # this is not really needed\n",
    "from torchvision.datasets import MNIST\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torch import nn, optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "# from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "## Necessary packages\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # 1. TimeGAN model\n",
    "# from timegan import timegan\n",
    "# # 2. Data loading\n",
    "# from data_loading import real_data_loading, sine_data_generation\n",
    "# # 3. Metrics\n",
    "# from metrics.discriminative_metrics import discriminative_score_metrics\n",
    "# from metrics.predictive_metrics import predictive_score_metrics\n",
    "# from metrics.visualization_metrics import visualization\n",
    "\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Hyper-parameters\n",
    "\n",
    "The cell below contains *all* the hyper-parameters nedded by this script, for easy tweaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1.0 # . . . Domain bounds for the loss functions\n",
    "M = 32 #. . . . Mini-batch size\n",
    "lr = 0.0007 # . Learning Rate\n",
    "k = 1.0 # . . . Regularization coefficient \n",
    "\n",
    "## Data loading\n",
    "data_name = 'stock' # . . which dataset to use\n",
    "seq_len = 24 #. . . . . . max length of the input sequence\n",
    "\n",
    "## Newtork parameters\n",
    "module = 'gru' #. . . . . Can be 'gru', 'lstm' or 'lstmLN'\n",
    "hidden_dim = 24 # . . . . Hidden dimensions\n",
    "num_layer = 3 # . . . . . Number of layers\n",
    "iterations = 10000 #. . . Number of epochs\n",
    "batch_size = 128 #. . . . Amount of samples in each batch\n",
    "\n",
    "metric_iteration = 5 #. . Number of iteration for each metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False # . . will require login for Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a function to count the number of parameters\n",
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "  \"\"\" Counts the number of trainable parameters of a module\n",
    "\n",
    "  :param model: model that contains the parameters to count\n",
    "  :returns: the number of parameters in the model\n",
    "  \"\"\"\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Initialization\n",
    "\n",
    "Initialize the modules needed by running the cells in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "_ = pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb:\n",
    "    !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_name in ['stock', 'energy']:\n",
    "  ori_data = real_data_loading(data_name, seq_len)\n",
    "elif data_name == 'sine':\n",
    "  # Set number of samples and its dimensions\n",
    "  no, dim = 10000, 5\n",
    "  ori_data = sine_data_generation(no, seq_len, dim)\n",
    "else:\n",
    "  assert(False)\n",
    "    \n",
    "print(data_name + ' dataset has been loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.4 Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataclass are fancy classes to hold data\n",
    "\n",
    "# Working with dataclasses is particularly comfortable\n",
    "# since you can specify types and get autocomplete/suggestion\n",
    "# of the available hyperparameters\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    #dataset_name: str = \"ukiyoe2photo\"  # name of the dataset\n",
    "\n",
    "    # Run we did:\n",
    "    # map: 200 epochs, 100 decay\n",
    "    # ukiyo: 40 epochs, 20 decay (due to time contraints)\n",
    "    # They took ~7 hours each on a 2080ti\n",
    "    n_epochs: int = 200  # number of epochs of training\n",
    "    decay_epoch: int = 100  # epoch from which to start lr decay\n",
    "\n",
    "    img_height: int = 128  # size of image height # default 256x256\n",
    "    img_width: int = 128  # size of image width\n",
    "\n",
    "    batch_size: int = 1  # size of the batches\n",
    "    lr: float = 0.0002  # adam: learning rate\n",
    "    b1: float = 0.5  # adam: decay of first order momentum of gradient\n",
    "    b2: float = 0.999  # adam: decay of first order momentum of gradient\n",
    "\n",
    "    channels: int = 3  # number of image channels\n",
    "    n_residual_blocks: int = 6  # number of residual blocks in generator # original 9\n",
    "    lambda_cyc: float = 10.0  # cycle loss weight\n",
    "    lambda_id: float = 5.0  # identity loss weight\n",
    "\n",
    "    n_cpu: int = 8  # number of cpu threads to use for the dataloaders\n",
    "\n",
    "    log_images: int = min(25, 100)  # number of images to log\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "##pprint(asdict(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Network Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.1.1 Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.1.2 Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.1.3 Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.1.4 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.1.5 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: Union[Dict, Config],\n",
    "        trainA_folder: Path,\n",
    "        trainB_folder: Path,\n",
    "        testA_folder: Path,\n",
    "        testB_folder: Path,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The CycleGAN model.\n",
    "\n",
    "        :param hparams: dictionary that contains all the hyperparameters\n",
    "        :param trainA_folder: Path to the folder that contains the trainA images\n",
    "        :param trainB_folder: Path to the folder that contains the trainB images\n",
    "        :param testA_folder: Path to the folder that contains the testA images\n",
    "        :param testB_folder: Path to the folder that contains the testB images\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(asdict(hparams) if not isinstance(hparams, Mapping) else hparams)\n",
    "\n",
    "        # Dataset paths\n",
    "        self.trainA_folder = trainA_folder\n",
    "        self.trainB_folder = trainB_folder\n",
    "        self.testA_folder = testA_folder\n",
    "        self.testB_folder = testB_folder\n",
    "\n",
    "        # Expected image shape\n",
    "        self.input_shape = (self.hparams[\"channels\"], self.hparams[\"img_height\"], self.hparams[\"img_width\"])\n",
    "\n",
    "        # Generators A->B and B->A\n",
    "        self.G_AB = GeneratorResNet(self.input_shape, self.hparams[\"n_residual_blocks\"])\n",
    "        self.G_BA = GeneratorResNet(self.input_shape, self.hparams[\"n_residual_blocks\"])\n",
    "\n",
    "        # Discriminators\n",
    "        self.D_A = Discriminator(self.input_shape)\n",
    "        self.D_B = Discriminator(self.input_shape)\n",
    "\n",
    "        # Initialize weights\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=nn%20module%20apply#torch.nn.Module.apply\n",
    "        self.G_AB.apply(self.weights_init_normal)\n",
    "        self.G_BA.apply(self.weights_init_normal)\n",
    "        self.D_A.apply(self.weights_init_normal)\n",
    "        self.D_B.apply(self.weights_init_normal)\n",
    "\n",
    "        # Image Normalizations\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(int(self.hparams[\"img_height\"] * 1.12), Image.BICUBIC),\n",
    "                transforms.RandomCrop((self.hparams[\"img_height\"], self.hparams[\"img_width\"])),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Image Normalization for the validation: remove source of randomness\n",
    "        self.val_image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(int(self.hparams[\"img_height\"] * 1.12), Image.BICUBIC),\n",
    "                transforms.CenterCrop((self.hparams[\"img_height\"], self.hparams[\"img_width\"])),\n",
    "                # transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Image buffers\n",
    "        self.fake_A_buffer = ReplayBuffer()\n",
    "        self.fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "        # Forward pass cache to avoid re-doing some computation\n",
    "        self.fake_A = None\n",
    "        self.fake_B = None\n",
    "\n",
    "        # Losses\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.l1 = torch.nn.L1Loss()\n",
    "\n",
    "        # Ignore this.\n",
    "        # It avoids wandb logging when lighting does a sanity check on the validation\n",
    "        self.is_sanity = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor, a_to_b: bool) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for this model.\n",
    "\n",
    "        This is not used while training!\n",
    "\n",
    "        :param x: input of the forward pass with shape [batch, channel, w, h]\n",
    "        :param a_to_b: if True uses the mapping A->B, otherwise uses B->A\n",
    "\n",
    "        :returns: the translated image with shape [batch, channel, w, h]\n",
    "        \"\"\"\n",
    "        if a_to_b:\n",
    "            return self.G_AB(x)\n",
    "        else:\n",
    "            return self.G_BA(x)\n",
    "\n",
    "    def weights_init_normal(self, m: nn.Module) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the weights with a gaussian N(0, 0.02) as described in the paper.\n",
    "\n",
    "        :param m: the module that contains the weights to initialise\n",
    "        \"\"\"\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find(\"Conv\") != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            if hasattr(m, \"bias\") and m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find(\"BatchNorm2d\") != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Create the train set DataLoader\n",
    "\n",
    "        :returns: the train set DataLoader\n",
    "        \"\"\"\n",
    "        train_loader = DataLoader(\n",
    "            DatasetUnpaired(\n",
    "                self.trainA_folder, self.trainB_folder, transform=self.image_transforms\n",
    "            ),\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self, custom_batch_size: Optional[int] = None) -> DataLoader:\n",
    "        \"\"\" Create the validation set DataLoader.\n",
    "\n",
    "        It is deterministic.\n",
    "        It does not shuffle and does not use random transformation on each image.\n",
    "\n",
    "        :returns: the validation set DataLoader\n",
    "        \"\"\"\n",
    "        test_loader = DataLoader(\n",
    "            DatasetUnpaired(\n",
    "                self.testA_folder,\n",
    "                self.testB_folder,\n",
    "                transform=self.val_image_transforms,\n",
    "                fixed_pairs=True,\n",
    "            ),\n",
    "            batch_size=custom_batch_size if custom_batch_size is not None else 32,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return test_loader\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Tuple[Sequence[optim.Optimizer], Sequence[Dict[str, Any]]]:\n",
    "        \"\"\" Instantiate the optimizers and schedulers.\n",
    "\n",
    "        We have three optimizers (and relative schedulers):\n",
    "\n",
    "        - Optimizer with index 0: optimizes the parameters of both generators\n",
    "        - Optimizer with index 1: optimizes the parameters of D_A\n",
    "        - Optimizer with index 2: optimizes the parameters of D_B\n",
    "\n",
    "        Each scheduler implements a linear decay to 0 after `cfg.hparams[\"decay_epoch\"]`\n",
    "\n",
    "        :returns: the optimizers and relative schedulers (look at the return type!)\n",
    "        \"\"\"\n",
    "        # Optimizers\n",
    "        optimizer_G = torch.optim.Adam(\n",
    "            itertools.chain(self.G_AB.parameters(), self.G_BA.parameters()),\n",
    "            lr=self.hparams[\"lr\"],\n",
    "            betas=(self.hparams[\"b1\"], self.hparams[\"b2\"]),\n",
    "        )\n",
    "        optimizer_D_A = torch.optim.Adam(\n",
    "            self.D_A.parameters(), lr=self.hparams[\"lr\"], betas=(self.hparams[\"b1\"], self.hparams[\"b2\"])\n",
    "        )\n",
    "        optimizer_D_B = torch.optim.Adam(\n",
    "            self.D_B.parameters(), lr=self.hparams[\"lr\"], betas=(self.hparams[\"b1\"], self.hparams[\"b2\"])\n",
    "        )\n",
    "\n",
    "        # Schedulers for each optimizers\n",
    "        lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer_G,\n",
    "            lr_lambda=LambdaLR(self.hparams[\"n_epochs\"], self.hparams[\"decay_epoch\"]).step,\n",
    "        )\n",
    "        lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer_D_A,\n",
    "            lr_lambda=LambdaLR(self.hparams[\"n_epochs\"], self.hparams[\"decay_epoch\"]).step,\n",
    "        )\n",
    "        lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer_D_B,\n",
    "            lr_lambda=LambdaLR(self.hparams[\"n_epochs\"], self.hparams[\"decay_epoch\"]).step,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            [optimizer_G, optimizer_D_A, optimizer_D_B],\n",
    "            [\n",
    "                {\"scheduler\": lr_scheduler_G, \"interval\": \"epoch\", \"frequency\": 1},\n",
    "                {\"scheduler\": lr_scheduler_D_A, \"interval\": \"epoch\", \"frequency\": 1},\n",
    "                {\"scheduler\": lr_scheduler_D_B, \"interval\": \"epoch\", \"frequency\": 1},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def criterion_GAN(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" The loss criterion for GAN losses\n",
    "\n",
    "        :param x: tensor with any shape\n",
    "        :param y: tensor with any shape\n",
    "\n",
    "        :returns: the mse between x and y\n",
    "        \"\"\"\n",
    "        return self.mse(x, y)\n",
    "\n",
    "    def criterion_cycle(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" The loss criterion for Cycle losses\n",
    "\n",
    "        :param x: tensor with any shape\n",
    "        :param y: tensor with any shape\n",
    "\n",
    "        :returns: the l1 between x and y\n",
    "        \"\"\"\n",
    "        return self.l1(x, y)\n",
    "\n",
    "    def criterion_identity(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" The loss criterion for Identity losses\n",
    "\n",
    "        :param x: tensor with any shape\n",
    "        :param y: tensor with any shape\n",
    "\n",
    "        :returns: the l1 between x and y\n",
    "        \"\"\"\n",
    "        return self.l1(x, y)\n",
    "\n",
    "    def identity_loss(self, image: torch.Tensor, generator: nn.Module) -> torch.Tensor:\n",
    "        \"\"\" Implements the identity loss for the given generator\n",
    "\n",
    "        :param generator: a generator module that maps X -> Y\n",
    "        :param image: an image in the Y distribution with shape [batch, channel, w, h]\n",
    "\n",
    "        :returns: the identity loss for these (generator, image)\n",
    "        \"\"\"\n",
    "        return self.criterion_identity(generator(image), image)\n",
    "\n",
    "    def gan_loss(\n",
    "        self,\n",
    "        generator: nn.Module,\n",
    "        discriminator: nn.Module,\n",
    "        image: torch.Tensor,\n",
    "        expected_label: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\" Implements the GAN loss for the given generator and discriminator\n",
    "\n",
    "        :param image: the input image with shape [batch, channle, w, h]\n",
    "        :param generator: the generator module to use to translate the image from X -> Y\n",
    "        :param discriminator: the discriminator that tries to distinguish fake and real images\n",
    "        :expected_label: tensor with shape compatible to the discriminator's output.\n",
    "                         It is full of ones when training the generator. We feed a fake\n",
    "                         image to the discriminator and we expect to get ones\n",
    "                         (for the discriminator this is an error!)\n",
    "\n",
    "        :returns: the GAN loss for these (image, generator, discriminator)\n",
    "        \"\"\"\n",
    "        fake_image = generator(image)\n",
    "        predicted_label = discriminator(fake_image)\n",
    "        loss_GAN = self.criterion_GAN(predicted_label, expected_label)\n",
    "        return loss_GAN, fake_image\n",
    "\n",
    "    def cycle_loss(\n",
    "        self,\n",
    "        fake_image: torch.Tensor,\n",
    "        reverse_generator: nn.Module,\n",
    "        original_image: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\" Implements the cycle consistency loss\n",
    "\n",
    "        It takes in input a fake image, to avoid repeated computation,\n",
    "        thus it only needs the reverse mapping that produced that fake image.\n",
    "\n",
    "        :param fake_image: a image produced by a mapping X->Y with shape [batch, channel, w, h]\n",
    "        :param reverse_generator: the generator module that maps Y->X\n",
    "        :param original_image: the original image in X with shape [batch, channel, w, h]\n",
    "                               to compare with the reconstructed fake image\n",
    "\n",
    "        :returns: the cycle consistency loss for this (fake_image, reverse_generator, original_image)\n",
    "        \"\"\"\n",
    "        recovered_image = reverse_generator(fake_image)\n",
    "        return self.criterion_cycle(recovered_image, original_image)\n",
    "\n",
    "    def discriminator_loss(\n",
    "        self,\n",
    "        discriminator: nn.Module,\n",
    "        proposed_image: torch.Tensor,\n",
    "        expected_label: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\" Implements the loss used to train the discriminator\n",
    "\n",
    "        :param discriminator: the discriminator model to train\n",
    "        :param proposed_image: the fake or real image proposed with shape [batch, channel, w, h]\n",
    "        :param expected_label: tensor with shape compatible to the discriminator's output,\n",
    "                               full of zeros if the proposed image is fake\n",
    "                               full of ones if the proposed image is real\n",
    "\n",
    "        :returns: the discriminator loss for this (discriminator, proposed_image, expected_label)\n",
    "        \"\"\"\n",
    "        predicted_label = discriminator(proposed_image)\n",
    "        return self.criterion_GAN(predicted_label, expected_label)\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Dict[str, torch.Tensor], batch_nb: int, optimizer_idx: int\n",
    "    ) -> Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]:\n",
    "        \"\"\" Implements a single training step\n",
    "\n",
    "        The parameter `optimizer_idx` identifies with optimizer \"called\" this training step,\n",
    "        this we can change the behaviour of the training depending on which optimizers\n",
    "        is currently performing the optimization\n",
    "\n",
    "        :param batch: current training batch\n",
    "        :param batch_nb: the index of the current batch\n",
    "        :param optimizer_idx: the index of the optimizer in use, see the function `configure_optimizers`\n",
    "\n",
    "        :returns: the total loss for the current training step, together with other information for the\n",
    "                  logging and possibly the progress bar\n",
    "        \"\"\"\n",
    "        # Unpack the batch\n",
    "        real_A = batch[\"A\"]\n",
    "        real_B = batch[\"B\"]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(\n",
    "            (real_A.size(0), *self.D_A.output_shape), device=real_A.device\n",
    "        )\n",
    "        fake = torch.zeros(\n",
    "            (real_A.size(0), *self.D_A.output_shape), device=real_A.device\n",
    "        )\n",
    "\n",
    "        # The first optimizer is for the two generators!\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # Identity A and B loss\n",
    "            loss_id_A = self.identity_loss(real_A, self.G_BA)\n",
    "            loss_id_B = self.identity_loss(real_B, self.G_AB)\n",
    "            loss_identity = self.hparams[\"lambda_id\"] * ((loss_id_A + loss_id_B) / 2)\n",
    "\n",
    "            # GAN A loss and GAN B loss\n",
    "            loss_GAN_AB, self.fake_B = self.gan_loss(self.G_AB, self.D_B, real_A, valid)\n",
    "            loss_GAN_BA, self.fake_A = self.gan_loss(self.G_BA, self.D_A, real_B, valid)\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle loss: A -> B -> A  and  B -> A -> B\n",
    "            loss_cycle_A = self.cycle_loss(self.fake_B, self.G_BA, real_A)\n",
    "            loss_cycle_B = self.cycle_loss(self.fake_A, self.G_AB, real_B)\n",
    "            loss_cycle = self.hparams[\"lambda_cyc\"] * ((loss_cycle_A + loss_cycle_B) / 2)\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + loss_cycle + loss_identity\n",
    "\n",
    "            self.log_dict({\n",
    "                    \"total_loss_generators\": loss_G,\n",
    "                    \"loss_GAN\": loss_GAN,\n",
    "                    \"loss_cycle\": loss_cycle,\n",
    "                    \"loss_identity\": loss_identity,\n",
    "                }\n",
    "            )\n",
    "            return loss_G\n",
    "\n",
    "        # The second optimizer is to train the D_A discriminator\n",
    "        elif optimizer_idx == 1:\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = self.discriminator_loss(self.D_A, real_A, valid)\n",
    "\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            loss_fake = self.discriminator_loss(\n",
    "                self.D_A, self.fake_A_buffer.push_and_pop(self.fake_A).detach(), fake\n",
    "            )\n",
    "\n",
    "            # Total loss\n",
    "            loss_D_A = (loss_real + loss_fake) / 2\n",
    "            self.log_dict({\n",
    "                    \"total_D_A\": loss_D_A,\n",
    "                    \"loss_D_A_real\": loss_real,\n",
    "                    \"loss_D_A_fake\": loss_fake,\n",
    "                }\n",
    "            )\n",
    "            return loss_D_A\n",
    "\n",
    "\n",
    "        # The second optimizer is to train the D_B discriminator\n",
    "        elif optimizer_idx == 2:\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = self.discriminator_loss(self.D_B, real_B, valid)\n",
    "\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            loss_fake = self.discriminator_loss(\n",
    "                self.D_B, self.fake_B_buffer.push_and_pop(self.fake_B).detach(), fake\n",
    "            )\n",
    "\n",
    "            # Total loss\n",
    "            loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            self.log_dict({\n",
    "                    \"total_D_B\": loss_D_B,\n",
    "                    \"loss_D_B_real\": loss_real,\n",
    "                    \"loss_D_B_fake\": loss_fake,\n",
    "                }\n",
    "            )\n",
    "            return loss_D_B\n",
    "\n",
    "        raise RuntimeError(\"There is an error in the optimizers configuration!\")\n",
    "\n",
    "    def get_image_examples(\n",
    "        self, real: torch.Tensor, fake: torch.Tensor\n",
    "    ) -> Sequence[wandb.Image]:\n",
    "        \"\"\"\n",
    "        Given real and \"fake\" translated images, produce a nice coupled images to log\n",
    "\n",
    "        :param real: the real images with shape [batch, channel, w, h]\n",
    "        :param fake: the fake image with shape [batch, channel, w, h]\n",
    "\n",
    "        :returns: a sequence of wandb.Image to log and visualize the performance\n",
    "        \"\"\"\n",
    "        example_images = []\n",
    "        for i in range(real.shape[0]):\n",
    "            couple = torchvision.utils.make_grid(\n",
    "                [real[i], fake[i]],\n",
    "                nrow=2,\n",
    "                normalize=True,\n",
    "                scale_each=True,\n",
    "                pad_value=1,\n",
    "                padding=4,\n",
    "            )\n",
    "            example_images.append(\n",
    "                wandb.Image(couple.permute(1, 2, 0).detach().cpu().numpy(), mode=\"RGB\")\n",
    "            )\n",
    "        return example_images\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Dict[str, torch.Tensor], batch_idx: int\n",
    "    ) -> Dict[str, Union[torch.Tensor,Sequence[wandb.Image]]]:\n",
    "        \"\"\" Implements a single validation step\n",
    "\n",
    "        In each validation step some translation examples are produced and a\n",
    "        validation loss that uses the cycle consistency is computed\n",
    "\n",
    "        :param batch: the current validation batch\n",
    "        :param batch_idx: the index of the current validation batch\n",
    "\n",
    "        :returns: the loss and example images\n",
    "        \"\"\"\n",
    "\n",
    "        real_B = batch[\"B\"]\n",
    "        fake_A = self.G_BA(real_B)\n",
    "        images_BA = self.get_image_examples(real_B, fake_A)\n",
    "\n",
    "        real_A = batch[\"A\"]\n",
    "        fake_B = self.G_AB(real_A)\n",
    "        images_AB = self.get_image_examples(real_A, fake_B)\n",
    "\n",
    "        ####\n",
    "\n",
    "        real_A = batch[\"A\"]\n",
    "        real_B = batch[\"B\"]\n",
    "\n",
    "        fake_B = self.G_AB(real_A)\n",
    "        fake_A = self.G_BA(real_B)\n",
    "\n",
    "        # Cycle loss A -> B -> A\n",
    "        recov_A = self.G_BA(fake_B)\n",
    "        loss_cycle_A = self.criterion_cycle(recov_A, real_A)\n",
    "\n",
    "        # Cycle loss B -> A -> B\n",
    "        recov_B = self.G_AB(fake_A)\n",
    "        loss_cycle_B = self.criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        # Cycle loss aggregation\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "        loss_cycle = self.hparams[\"lambda_cyc\"] * loss_cycle\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_cycle\n",
    "\n",
    "        return {\"val_loss\": loss_G, \"images_BA\": images_BA, \"images_AB\": images_AB}\n",
    "\n",
    "    def validation_epoch_end(\n",
    "        self, outputs: List[Dict[str, torch.Tensor]]\n",
    "    ) -> Dict[str, Union[torch.Tensor, Dict[str, Union[torch.Tensor,Sequence[wandb.Image]]]]]:\n",
    "        \"\"\" Implements the behaviouir at the end of a validation epoch\n",
    "\n",
    "        Currently it gathers all the produced examples and log them to wandb,\n",
    "        limiting the logged examples to `hparams[\"log_images\"]`.\n",
    "\n",
    "        Then computes the mean of the losses and returns it.\n",
    "        Updates the progress bar label with this loss.\n",
    "\n",
    "        :param outputs: a sequence that aggregates all the outputs of the validation steps\n",
    "\n",
    "        :returns: the aggregated validation loss and information to update the progress bar\n",
    "        \"\"\"\n",
    "        images_AB = []\n",
    "        images_BA = []\n",
    "\n",
    "        for x in outputs:\n",
    "            images_AB.extend(x[\"images_AB\"])\n",
    "            images_BA.extend(x[\"images_BA\"])\n",
    "\n",
    "        images_AB = images_AB[: self.hparams[\"log_images\"]]\n",
    "        images_BA = images_BA[: self.hparams[\"log_images\"]]\n",
    "\n",
    "        if not self.is_sanity:  # ignore if it not a real validation epoch. The first one is not.\n",
    "            print(f\"Logged {len(images_AB)} images for each category.\")\n",
    "\n",
    "            self.logger.experiment.log(\n",
    "                {f\"images_AB\": images_AB, f\"images_BA\": images_BA,},\n",
    "                step=self.global_step,\n",
    "            )\n",
    "        self.is_sanity = False\n",
    "\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        self.log_dict({\"val_loss\": avg_loss})\n",
    "        return {\"val_loss\": avg_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Train\n",
    "\n",
    "This chapter will train the model according to the hyper-parameters defined above in section [Hyper-parameters](#13-hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⏱⏱⏱ slow executing cell ⏱⏱⏱\n",
    "# Suggested to use pre-trained models!\n",
    "\n",
    "# Instantiate the model\n",
    "gan_model = CycleGAN(hparams=cfg,\n",
    "                     trainA_folder=trainA,\n",
    "                     trainB_folder=trainB,\n",
    "                     testA_folder=testA,\n",
    "                     testB_folder=testB)\n",
    "\n",
    "# Define the logger\n",
    "# https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n",
    "wandb_logger = WandbLogger(project=\"CycleGAN Tutorial 2021\", log_model=True)\n",
    "\n",
    "## Currently it does not log the model weights, there is a bug in wandb and/or lightning.\n",
    "wandb_logger.experiment.watch(gan_model, log='all', log_freq=100)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = pl.Trainer(logger=wandb_logger,\n",
    "                     max_epochs=cfg.n_epochs,\n",
    "                     gpus=1,\n",
    "                     limit_val_batches=.2,\n",
    "                     val_check_interval=0.25)\n",
    "\n",
    "# Start the training\n",
    "trainer.fit(gan_model)\n",
    "\n",
    "# Log the trained model\n",
    "trainer.save_checkpoint('model.pth')\n",
    "wandb.save('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
